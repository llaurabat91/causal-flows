{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce6deb5c-72f2-4660-a00f-aff2c3a9642c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/llaurabat/micromamba/envs/causal-flows/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import causal_nf.utils.wandb_local as wandb_local\n",
    "import causal_nf.config as causal_nf_config\n",
    "from causal_nf.config import cfg\n",
    "import causal_nf.utils.training as causal_nf_train\n",
    "import os\n",
    "import glob\n",
    "import argparse\n",
    "from yacs.config import CfgNode \n",
    "import torch\n",
    "import causal_nf.utils.io as causal_nf_io\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df864a36-1129-4049-9e50-58bfbee4638e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from causal_nf.preparators.FF_preparator import FFPreparator\n",
    "from causal_nf.config import cfg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a956ffd-5e6d-44bd-89e7-2f2a7795b021",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ckpt_file = \"/Users/llaurabat/Dropbox/Oxford/deep_copula_frugal/causal-flows/output_FF_final/edf3624c525411ef8eecacde48001122/last.ckpt\"\n",
    "# ckpt_file = \"/Users/llaurabat/Dropbox/Oxford/deep_copula_frugal/causal-flows/output_FF_final_adj_ON/1f995ea8525c11efbcefacde48001122/last.ckpt\"\n",
    "# ckpt_file = \"/Users/llaurabat/Dropbox/Oxford/deep_copula_frugal/causal-flows/output_FF_final_ate1/a6cbbd08526111efa71facde48001122/last.ckpt\"\n",
    "ckpt_file= \"/Users/llaurabat/Dropbox/Oxford/deep_copula_frugal/causal-flows/output_FF_final_ate1_check/9d16ffae528211ef985aacde48001122/last.ckpt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ce410bc-b3cd-470b-8191-678d803c8d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# args_list = []\n",
    "# args =  CfgNode({'config_file': 'output_FF_final/edf3624c525411ef8eecacde48001122/wandb_local/config_local.yaml', \n",
    "#                  'config_default_file': 'output_FF_final/edf3624c525411ef8eecacde48001122/wandb_local/default_config.yaml', \n",
    "#                  'project': None, 'wandb_mode': 'disabled', 'wandb_group': None, \n",
    "#                  'load_model': 'output_FF_final/edf3624c525411ef8eecacde48001122', 'delete_ckpt': False})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6002e13-2e8c-49ef-85fb-b8f53404707d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# args_list = []\n",
    "# args =  CfgNode({'config_file': 'output_FF_final_adj_ON/1f995ea8525c11efbcefacde48001122/wandb_local/config_local.yaml', \n",
    "#                  'config_default_file': 'output_FF_final_adj_ON/1f995ea8525c11efbcefacde48001122/wandb_local/default_config.yaml', \n",
    "#                  'project': None, 'wandb_mode': 'disabled', 'wandb_group': None, \n",
    "#                  'load_model': 'output_FF_final_adj_ON/1f995ea8525c11efbcefacde48001122', 'delete_ckpt': False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b578015-e919-4082-a906-d0d714b2c9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# args_list = []\n",
    "# args =  CfgNode({'config_file': 'output_FF_final_ate1/a6cbbd08526111efa71facde48001122/wandb_local/config_local.yaml', \n",
    "#                  'config_default_file': 'output_FF_final_ate1/a6cbbd08526111efa71facde48001122/wandb_local/default_config.yaml', \n",
    "#                  'project': None, 'wandb_mode': 'disabled', 'wandb_group': None, \n",
    "#                  'load_model': 'output_FF_final_ate1/a6cbbd08526111efa71facde48001122', 'delete_ckpt': False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6f05be1-45de-4346-ab24-ed856e94fc46",
   "metadata": {},
   "outputs": [],
   "source": [
    "args_list = []\n",
    "args =  CfgNode({'config_file': 'output_FF_final_ate1_check/9d16ffae528211ef985aacde48001122/wandb_local/config_local.yaml', \n",
    "                 'config_default_file': 'output_FF_final_ate1_check/9d16ffae528211ef985aacde48001122/wandb_local/default_config.yaml', \n",
    "                 'project': None, 'wandb_mode': 'disabled', 'wandb_group': None, \n",
    "                 'load_model': 'output_FF_final_ate1_check/9d16ffae528211ef985aacde48001122', 'delete_ckpt': False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "596e899b-7852-43c3-8d70-abe423fccf8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# args_list, args = causal_nf_config.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "862f1d92-ab34-4b90-ab7a-d0ee61bd4bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = causal_nf_config.build_config(\n",
    "    config_file=args.config_file,\n",
    "    args_list=args_list,\n",
    "    config_default_file=args.config_default_file,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "96ff3734-710b-410b-9987-b63319cd911c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CfgNode({'root': '../Data', 'name': 'FF_data', 'sem_name': 'dummy', 'splits': [0.8, 0.1, 0.1], 'k_fold': 1, 'shuffle_train': True, 'single_split': False, 'loss': 'default', 'scale': 'default', 'num_samples': 1000, 'base_version': 1, 'add_noise': False, 'type': 'torch', 'use_edge_attr': False})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f748560d-65c1-4942-923e-8544e84884ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "causal_nf_config.assert_cfg_and_config(cfg, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "61ef2e8f-8dcf-4787-ae4f-b528abcafd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "preparator = FFPreparator.loader(cfg.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f0125900-5e6c-4ec9-b7bb-ab346293db7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preparing data...\n",
      "\n",
      "Preparing data...\n",
      "\n",
      "Preparing data...\n"
     ]
    }
   ],
   "source": [
    "preparator.prepare_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b24d42d8-f39d-4421-bc40-56d9853b5c7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NSF(\n",
      "  (transforms): ModuleList(\n",
      "    (0): MaskedAutoregressiveTransform(\n",
      "      (base): MonotonicRQSTransform(bins=8)\n",
      "      (order): [0, 1, 2, 3, 4, 5]\n",
      "      (hyper): MaskedMLP(\n",
      "        (0): MaskedLinear(in_features=6, out_features=32, bias=True)\n",
      "        (1): ELU(alpha=1.0)\n",
      "        (2): MaskedLinear(in_features=32, out_features=32, bias=True)\n",
      "        (3): ELU(alpha=1.0)\n",
      "        (4): MaskedLinear(in_features=32, out_features=32, bias=True)\n",
      "        (5): ELU(alpha=1.0)\n",
      "        (6): MaskedLinear(in_features=32, out_features=138, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (base): DiagNormal(loc: torch.Size([6]), scale: torch.Size([6]))\n",
      ")\n",
      "\u001b[94m[INFO] Loading <class 'causal_nf.models.causal_nf.CausalNFightning'> from /Users/llaurabat/Dropbox/Oxford/deep_copula_frugal/causal-flows/output_FF_final_ate1_check/9d16ffae528211ef985aacde48001122/last.ckpt\u001b[0m\n",
      "scaler_transform StandardTransform(shift=tensor([2.7234, 2.7408, 2.7082, 2.6903, 0.9523, 1.9587]), scale=tensor([2.7295, 2.6963, 2.7701, 2.7185, 0.2133, 1.0559]))\n",
      "StandardScaler(\n",
      "\t\tmu_=tensor([[2.7234, 2.7408, 2.7082, 2.6903, 0.9523, 1.9587]]), \n",
      "\t\tscale_=tensor([[2.7295, 2.6963, 2.7701, 2.7185, 0.2133, 1.0559]])\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# IMPORTANT: insert ckpt file otherwise it loads anyway bt something wrong\n",
    "model_lightning = causal_nf_train.load_model(cfg=cfg, preparator=preparator, ckpt_file=ckpt_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f915d719-e1db-4682-b9b1-7c4d005d928e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(model.model.named_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "65257009-bebd-4ca1-9b74-b8f0d511e615",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CausalNormalizingFlow(\n",
       "  (flow): NSF(\n",
       "    (transforms): ModuleList(\n",
       "      (0): MaskedAutoregressiveTransform(\n",
       "        (base): MonotonicRQSTransform(bins=8)\n",
       "        (order): [0, 1, 2, 3, 4, 5]\n",
       "        (hyper): MaskedMLP(\n",
       "          (0): MaskedLinear(in_features=6, out_features=32, bias=True)\n",
       "          (1): ELU(alpha=1.0)\n",
       "          (2): MaskedLinear(in_features=32, out_features=32, bias=True)\n",
       "          (3): ELU(alpha=1.0)\n",
       "          (4): MaskedLinear(in_features=32, out_features=32, bias=True)\n",
       "          (5): ELU(alpha=1.0)\n",
       "          (6): MaskedLinear(in_features=32, out_features=138, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (base): DiagNormal(loc: torch.Size([6]), scale: torch.Size([6]))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = model_lightning.model\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "150edbd7-811d-46c8-8de9-02646141746b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94m[INFO] [0] num_batchees: 1\u001b[0m\n",
      "\u001b[94m[INFO] [1] num_batchees: 1\u001b[0m\n",
      "\u001b[94m[INFO] [2] num_batchees: 1\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "loaders = preparator.get_dataloaders(\n",
    "    batch_size=cfg.train.batch_size, num_workers=cfg.train.num_workers\n",
    ")\n",
    "\n",
    "for i, loader in enumerate(loaders):\n",
    "    causal_nf_io.print_info(f\"[{i}] num_batchees: {len(loader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cae3374a-410a-457f-9548-402a3f69e97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(loaders[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "92623fdb-a28a-4c7c-8ed5-3d3bab2c9c99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 4.1480e+00,  2.0225e-01,  1.7911e+00, -1.0034e+00,  2.1222e+00,\n",
       "         1.2361e+00,  3.3899e+00,  2.1877e+00,  2.1903e+00,  1.0463e+00,\n",
       "         3.3979e+00,  2.5997e+00,  3.3265e+00,  1.9016e+00,  2.4261e-01,\n",
       "         1.6047e+00,  2.1983e+00,  1.5351e+00,  9.1221e-01,  9.4868e-01,\n",
       "         2.1077e+00,  2.3309e+00,  7.7667e-01,  3.8567e-01,  1.1713e+00,\n",
       "         6.5297e-01,  3.6031e-01,  2.5990e+00,  2.3453e+00,  2.6452e+00,\n",
       "         2.1796e+00,  2.4353e+00,  3.1512e-01,  1.1139e+00,  2.5765e+00,\n",
       "         9.7517e-02,  3.6379e+00,  1.3688e+00,  2.4686e+00,  3.9479e+00,\n",
       "         3.7692e-01,  3.1133e+00,  3.0773e+00,  5.0698e-01,  5.7148e-01,\n",
       "         1.8828e+00,  2.1106e+00,  2.3609e+00,  3.1870e+00,  1.1142e+00,\n",
       "         1.8064e+00,  2.1699e+00,  2.4277e+00,  1.9419e+00,  3.5183e+00,\n",
       "         1.7785e+00,  3.9868e-01,  2.1339e+00,  9.9808e-01,  1.4953e+00,\n",
       "         3.7664e+00,  3.8612e+00,  4.0687e+00,  2.1283e+00,  8.5413e-01,\n",
       "         4.2193e-01,  1.8101e+00,  1.1327e+00, -6.6089e-01,  1.9770e+00,\n",
       "         1.2413e+00,  2.6744e+00,  3.0137e+00,  3.7243e+00,  3.4627e+00,\n",
       "         2.1543e+00,  3.2106e+00,  1.3628e+00,  4.3723e-01,  3.1929e+00,\n",
       "         3.3799e+00,  4.4430e+00,  2.6983e+00,  1.7217e+00,  4.1346e+00,\n",
       "         3.2961e+00,  1.5919e+00,  1.6372e+00,  3.2285e+00,  4.6335e+00,\n",
       "         2.1779e+00,  9.2687e-01,  2.7208e+00,  3.3335e+00,  3.4697e+00,\n",
       "         1.8344e+00,  2.1698e+00,  1.2552e+00,  1.6328e+00,  2.3243e+00,\n",
       "         1.1921e+00,  2.0267e+00,  3.1710e+00,  9.2354e-01,  2.5334e+00,\n",
       "         1.8278e+00,  3.3631e+00,  1.4514e+00,  1.7109e+00,  1.9463e+00,\n",
       "         1.7038e+00,  3.8236e+00,  1.0473e+00,  1.1570e+00,  2.2474e+00,\n",
       "         8.3569e-01,  3.2996e+00,  3.6181e+00,  1.8837e+00,  4.2411e+00,\n",
       "         2.1662e+00,  4.5733e+00,  1.7018e+00,  2.2228e+00,  1.5559e+00,\n",
       "         2.4813e+00,  4.3191e+00,  2.5919e+00,  3.7053e+00,  2.7475e+00,\n",
       "        -7.7768e-01,  2.3668e+00,  2.6712e+00,  1.3543e+00,  2.7864e-01,\n",
       "         2.6944e+00,  2.0626e+00,  2.7937e+00,  2.3848e+00,  3.0934e+00,\n",
       "         4.0763e+00,  3.1812e+00,  4.1320e+00,  2.0507e+00,  7.0762e-01,\n",
       "        -1.3221e-01,  3.0037e+00,  2.9205e+00,  5.5517e-01, -3.5050e-01,\n",
       "         1.7212e+00,  9.2205e-01,  9.3572e-01,  2.4269e+00,  3.3730e+00,\n",
       "         1.4778e+00,  3.7064e+00,  2.1940e+00,  1.2926e+00, -7.5161e-01,\n",
       "         7.1050e-01,  2.4540e+00,  1.4982e+00, -2.0528e-02,  1.9030e+00,\n",
       "         1.3497e+00,  4.3047e-01,  3.9160e+00,  4.3783e+00, -1.1501e-01,\n",
       "         1.9073e+00,  8.2511e-01,  2.2488e+00,  2.1616e+00,  2.4012e+00,\n",
       "        -1.1678e+00,  3.4670e+00,  9.3686e-01,  1.8117e+00,  2.0827e+00,\n",
       "         1.7853e+00,  1.8395e+00,  2.6267e+00,  9.1726e-01, -6.7477e-01,\n",
       "         1.8525e+00,  2.9200e+00,  3.5585e+00,  7.3788e-01,  3.4214e+00,\n",
       "         3.8668e+00,  2.2519e+00,  2.6950e+00,  2.0541e+00,  3.5703e+00,\n",
       "         3.8418e+00,  2.3179e+00,  3.7046e+00,  9.3214e-01,  1.7707e+00,\n",
       "         1.8471e+00,  9.6520e-01,  1.1945e+00,  1.6714e+00,  9.4798e-01,\n",
       "         1.9102e+00,  2.3535e+00,  3.7730e+00,  1.3960e+00,  1.9751e+00,\n",
       "         7.9327e-01,  7.3357e-02,  1.5621e+00,  1.6786e+00,  2.8991e+00,\n",
       "         1.3482e+00,  1.9550e+00,  2.4545e+00,  3.0049e+00,  2.5799e+00,\n",
       "         2.1509e+00,  3.2652e+00,  3.5947e+00, -1.2782e+00,  3.5301e-01,\n",
       "         1.1587e+00,  2.2953e+00,  4.3378e+00,  1.6918e+00,  2.4855e+00,\n",
       "         3.6997e+00,  2.7054e+00,  2.1540e+00,  1.9004e+00,  2.3614e+00,\n",
       "         3.6139e+00,  2.5472e+00,  2.7328e+00,  1.6793e+00,  3.2446e+00,\n",
       "         2.5116e+00,  3.0438e+00,  5.5724e-01,  2.6023e+00,  2.5739e+00,\n",
       "         1.6669e+00,  3.6098e+00,  1.0014e+00,  3.0042e+00,  2.5617e+00,\n",
       "         3.0881e+00,  5.7634e-01,  1.3289e+00,  2.3598e+00,  2.2295e+00,\n",
       "        -8.9433e-01,  2.9974e+00,  2.0910e+00,  2.9816e+00,  3.5987e+00,\n",
       "         7.3786e-01,  1.4529e-01,  1.4749e+00,  1.5257e+00, -1.4494e+00,\n",
       "         1.3471e+00,  4.3742e-01,  2.0013e+00,  1.2180e+00,  2.7775e+00,\n",
       "         2.5274e+00,  1.8479e+00,  2.8290e+00,  6.1072e-01,  1.6024e+00,\n",
       "         2.8218e+00,  3.5774e+00,  1.4396e+00, -9.0711e-01,  1.8835e+00,\n",
       "         1.2499e+00,  9.0075e-01,  8.8254e-01,  1.7458e+00,  1.0688e+00,\n",
       "         2.3518e+00,  1.4575e+00,  2.2436e+00,  2.4217e+00,  2.7068e+00,\n",
       "         1.5650e+00,  2.6981e+00,  9.1491e-01,  2.9967e+00, -2.2622e-01,\n",
       "         1.3460e+00,  3.3321e+00,  1.4045e+00,  9.9226e-01,  1.6517e+00,\n",
       "         1.6427e+00,  1.9430e+00,  2.0592e+00,  1.0104e+00,  1.6994e+00,\n",
       "         3.0742e+00,  2.4724e+00,  2.9207e+00,  1.5033e+00,  1.6364e+00,\n",
       "         2.3298e+00, -1.9072e-01,  3.2944e+00,  9.4392e-01,  1.6949e+00,\n",
       "         3.5725e-01,  3.3332e+00,  2.8559e+00,  3.6827e+00,  1.4003e+00,\n",
       "         1.7438e+00,  2.1882e+00,  1.9399e-01,  3.4905e-01,  1.7621e+00,\n",
       "         9.4246e-02,  3.3718e+00, -7.9632e-02,  2.4235e-01,  2.0108e+00,\n",
       "         7.7409e-01,  2.7243e+00,  1.7542e+00,  2.8314e+00,  4.1341e-01,\n",
       "         1.9267e+00,  1.8699e+00,  2.1684e+00,  1.5483e+00,  1.7570e+00,\n",
       "         2.4173e+00,  1.7514e+00,  2.2212e+00,  2.3536e-01,  2.0885e+00,\n",
       "         1.3193e+00,  2.6179e+00,  4.7302e-01,  1.7169e+00,  1.7080e+00,\n",
       "         2.9316e+00,  2.1052e+00,  2.5926e+00,  1.7422e+00,  2.5542e+00,\n",
       "         1.5101e+00,  1.2604e+00,  1.3579e+00,  4.7441e-02,  1.9808e+00,\n",
       "         2.4668e+00,  2.4433e+00,  3.8642e+00,  3.6492e-01,  7.8346e-01,\n",
       "         2.6488e+00,  2.0426e+00,  2.1216e+00,  1.7728e+00,  2.0075e+00,\n",
       "         1.9640e+00,  3.7989e+00,  2.2803e+00,  2.4781e+00, -1.9204e+00,\n",
       "         3.0552e+00,  7.4839e-01,  7.1108e-01,  3.5727e+00,  1.9602e+00,\n",
       "         3.4285e+00,  5.3249e-01,  2.4447e+00,  1.9555e+00,  1.2109e+00,\n",
       "         9.2379e-01,  1.9839e+00,  1.9420e+00,  1.3665e+00,  1.7356e+00,\n",
       "         2.3062e+00,  4.0274e+00,  1.8781e+00,  1.9930e+00,  1.3658e+00,\n",
       "         1.7581e+00,  1.9402e+00,  4.2440e+00,  3.8026e+00,  3.6346e+00,\n",
       "         1.7081e+00,  2.6905e+00,  1.0832e+00,  3.5278e+00,  3.7138e+00,\n",
       "         2.8570e+00,  2.7838e+00,  2.9119e+00,  4.9712e+00,  4.7574e+00,\n",
       "         3.2932e+00,  2.4948e-01,  2.1946e+00,  1.8315e+00,  1.6915e+00,\n",
       "         3.3885e+00,  8.4009e-01,  2.2316e+00,  2.0076e+00,  9.3020e-02,\n",
       "         2.2356e+00,  2.9884e+00,  2.8018e+00,  2.3349e+00,  2.8209e+00,\n",
       "         2.7792e+00,  2.5410e+00,  2.9136e+00,  1.2124e+00,  1.2830e+00,\n",
       "         1.8655e+00,  2.0368e-01,  1.0190e+00,  1.4819e-01,  2.0117e+00,\n",
       "         2.4820e+00,  3.5838e+00,  1.9639e+00,  1.2112e+00,  1.4851e+00,\n",
       "         1.9795e+00,  9.9086e-01,  1.5027e+00,  1.9379e+00,  1.1250e+00,\n",
       "         1.9692e+00,  2.4985e+00,  3.7382e+00,  2.9303e+00,  1.1977e+00,\n",
       "         1.3826e+00,  1.1887e+00,  3.3357e+00,  3.5234e+00,  2.0643e+00,\n",
       "         1.6666e+00,  3.5845e+00,  2.5343e+00, -6.3638e-01,  3.6700e+00,\n",
       "         3.5264e+00,  4.4026e-01,  8.5984e-01,  2.9870e+00,  1.8049e+00,\n",
       "         1.8218e+00, -6.8530e-01,  2.7932e+00,  3.5209e-01,  2.8894e+00,\n",
       "        -5.8991e-01,  1.7824e+00,  1.1327e+00,  2.7356e+00,  3.0769e+00,\n",
       "         1.9041e+00,  2.7621e+00, -7.0865e-01,  2.4937e+00,  1.4925e+00,\n",
       "         3.3238e+00,  1.5485e+00,  1.7210e+00,  1.8809e+00,  2.3806e+00,\n",
       "         1.3132e+00,  2.7337e+00,  2.5309e+00,  2.4884e+00,  2.4098e+00,\n",
       "         2.4079e+00,  7.9753e-02,  2.9371e+00,  1.9271e+00,  1.2975e+00,\n",
       "         3.3954e+00,  2.2400e+00,  1.1240e+00,  1.4605e+00,  1.0645e+00,\n",
       "         1.8822e+00,  3.6980e+00,  1.7260e+00,  1.8696e+00,  2.5535e+00,\n",
       "         2.4394e+00,  2.1909e+00,  3.0839e+00,  1.7237e+00, -1.1443e+00,\n",
       "         8.7812e-01,  3.4263e-01,  2.3195e+00,  2.3787e+00,  2.5120e+00,\n",
       "         3.0001e+00,  1.6080e+00,  2.9980e+00,  3.1341e-01,  4.2290e+00,\n",
       "         1.6222e+00,  2.3045e+00,  3.7669e+00,  1.5846e+00,  2.0384e+00,\n",
       "         2.5711e+00,  2.6281e+00,  3.6957e+00,  2.9966e+00,  2.5565e+00,\n",
       "         1.9628e+00, -1.3730e+00,  2.6152e+00,  2.6869e+00,  2.7544e+00,\n",
       "         2.5170e+00,  3.8281e-01,  2.3187e+00,  1.8762e+00,  2.7467e+00,\n",
       "         1.3609e+00,  3.2791e+00,  2.6337e+00,  1.0398e+00,  1.9511e+00,\n",
       "         2.3028e+00,  1.2360e+00,  3.3953e+00,  1.5996e+00,  2.5620e+00,\n",
       "         3.3268e+00,  2.2358e+00,  3.1153e+00,  2.6075e+00,  2.7534e+00,\n",
       "         1.2268e+00,  1.8152e+00,  1.4732e+00,  1.5837e+00,  1.3574e-01,\n",
       "         1.9022e+00,  1.4437e+00,  1.9278e+00,  1.2903e+00,  3.6254e+00,\n",
       "         1.4982e+00,  2.3275e+00,  1.9754e+00,  1.5121e+00,  2.0230e+00,\n",
       "         3.0084e-01,  2.1510e+00, -3.2439e-03,  4.1142e+00,  4.0589e-01,\n",
       "         1.9191e+00,  2.5941e+00,  3.5294e+00,  6.7179e-01,  1.4930e+00,\n",
       "         1.8308e+00,  2.3774e-01,  9.8125e-01,  2.1120e+00, -1.0208e-01,\n",
       "         3.0151e+00,  3.7482e+00,  9.6398e-01,  2.4580e+00,  1.6569e+00,\n",
       "         1.8586e+00,  2.0882e+00,  2.2607e+00,  2.1452e-01,  4.8404e+00,\n",
       "         8.1842e-01,  3.2623e+00,  9.4113e-01,  3.3375e+00,  2.7720e+00,\n",
       "         3.2960e+00,  3.0170e+00,  9.2532e-01,  1.4548e+00,  2.2068e+00,\n",
       "         1.9291e+00,  1.0327e+00,  4.2771e+00,  2.7910e+00, -1.1358e+00,\n",
       "         2.2401e+00,  2.8458e+00,  1.9222e+00,  2.5683e+00,  1.6210e+00,\n",
       "         1.5286e+00,  3.5692e+00,  4.0542e+00,  1.3280e-01,  1.4146e+00,\n",
       "         2.3277e+00,  1.7944e+00,  2.5181e+00,  2.2216e+00,  1.7516e+00,\n",
       "         2.2128e+00,  8.9685e-01,  1.6526e+00,  1.0717e+00,  5.8010e-01,\n",
       "         2.8622e+00,  1.1396e+00,  1.6743e+00,  2.4382e+00,  1.0152e+00,\n",
       "        -1.3425e-01,  1.9692e+00,  1.1282e+00,  7.3908e-01,  3.6883e+00,\n",
       "        -5.1844e-01,  2.0017e+00,  2.3024e+00,  2.1694e+00,  2.4360e+00,\n",
       "         2.2563e+00,  3.0782e+00,  1.6883e+00,  2.7654e+00,  7.9267e-01,\n",
       "         1.7567e+00,  1.5576e+00,  2.7241e+00,  2.6041e+00,  3.5459e+00,\n",
       "         2.8808e+00,  2.3556e+00,  1.0161e-01,  1.8078e+00,  1.0098e+00,\n",
       "         2.0694e+00,  1.7846e+00,  2.0707e+00,  8.3006e-01,  3.0751e+00,\n",
       "         1.7548e+00,  3.0040e+00,  6.4620e-01,  1.6995e+00,  1.9315e+00,\n",
       "         8.5156e-01,  1.6507e+00,  1.6325e+00,  1.7850e+00,  2.6761e+00,\n",
       "         1.8683e+00,  2.6374e+00,  6.1525e-01,  1.8425e+00,  2.8652e+00,\n",
       "         2.6006e+00,  2.5834e+00,  1.8707e+00,  2.2126e+00,  2.5075e+00,\n",
       "         8.6782e-01,  4.6895e+00,  3.5348e+00,  1.7136e+00,  2.1093e+00,\n",
       "         2.8177e+00,  1.8032e+00,  1.9499e+00,  2.9375e+00,  1.3026e+00,\n",
       "         2.0490e+00,  1.4418e+00,  2.2744e+00,  2.2666e+00,  3.9750e+00,\n",
       "         3.0054e+00,  1.8870e+00,  2.2771e+00,  2.9184e+00,  2.7819e+00,\n",
       "         3.1846e+00,  2.4181e+00,  1.7812e+00,  2.2083e+00,  2.4965e+00,\n",
       "         2.0292e+00,  1.0868e+00,  2.6347e+00,  3.7587e+00,  3.0024e+00,\n",
       "         1.9868e+00,  3.1662e+00,  1.3777e+00,  2.0873e+00,  2.0764e+00,\n",
       "         3.3561e+00,  1.6228e+00, -1.7262e-01,  1.8446e+00,  8.0621e-01,\n",
       "         2.6787e+00,  3.1967e+00,  3.9611e+00,  2.4272e+00,  2.9809e+00,\n",
       "         9.3034e-01,  2.2025e+00,  1.3991e+00,  6.0290e-01,  5.1154e-01,\n",
       "         3.6077e+00, -7.2915e-01,  1.9382e+00,  3.7836e+00,  2.4859e+00,\n",
       "         1.9995e+00,  1.6673e+00,  2.1667e+00,  2.2676e+00,  1.7086e+00,\n",
       "         3.1237e+00,  1.5405e+00,  3.1518e+00,  1.1774e+00,  2.5565e+00,\n",
       "         2.5664e+00,  2.0685e+00,  1.5941e+00,  1.0302e+00,  1.4748e+00,\n",
       "         2.6723e+00,  2.3931e+00, -1.0775e+00,  1.9963e+00,  1.9189e+00,\n",
       "         1.7008e+00,  3.8044e+00,  7.9309e-01,  9.1913e-02,  2.0573e+00,\n",
       "         3.0191e+00,  2.8295e+00,  2.8824e+00, -9.3746e-01,  3.5112e+00,\n",
       "         1.2079e+00,  1.9552e+00,  2.3610e+00,  2.0698e+00,  2.1711e+00,\n",
       "         2.6416e+00,  2.3747e+00,  2.5795e+00, -7.3593e-01,  3.9306e+00,\n",
       "         1.6413e+00,  2.3043e+00, -7.9807e-02,  1.5810e+00,  1.6350e+00,\n",
       "         9.9845e-01,  3.0059e+00,  3.1335e+00,  1.4261e+00,  2.9671e+00,\n",
       "         2.8602e+00,  1.4930e+00,  1.6544e+00,  2.1792e+00,  2.8532e+00,\n",
       "         1.6060e+00,  9.3719e-01,  1.5999e+00,  2.4552e+00,  2.6059e+00,\n",
       "         2.0651e+00,  2.1826e+00,  3.1669e+00,  1.9910e+00,  3.0971e+00,\n",
       "         1.0362e+00,  1.8718e+00,  1.4404e+00,  3.4488e+00,  6.5678e-01,\n",
       "         2.6551e+00,  2.7148e+00,  1.1819e+00,  2.7784e+00,  1.8227e-01,\n",
       "         1.7541e+00,  2.1130e+00,  3.4819e+00,  5.1101e-01,  1.6783e+00,\n",
       "         3.3212e+00,  3.0342e+00,  2.0190e+00,  2.7565e-01,  2.4002e+00,\n",
       "         4.7291e+00,  1.3831e+00,  2.7932e+00,  2.6778e-01,  2.2309e+00,\n",
       "         2.3221e+00,  1.5290e+00,  1.5604e+00,  2.9739e+00,  1.5366e+00,\n",
       "         2.8679e+00,  2.1788e+00,  2.9061e+00,  1.7564e+00,  3.2633e+00,\n",
       "         2.3859e+00,  3.3696e+00,  3.1483e+00, -6.6973e-01,  2.9509e+00,\n",
       "         1.5486e+00,  2.7076e+00,  2.4918e+00,  1.7342e+00,  2.0714e+00,\n",
       "         1.3906e+00,  2.1122e+00,  2.6162e+00,  4.3869e-01, -6.8367e-01,\n",
       "         2.2746e+00,  2.4884e+00,  2.7967e+00,  4.5992e-01,  7.0601e-01,\n",
       "         1.9403e+00,  3.2335e-01,  1.9973e+00,  1.3550e+00,  2.4364e+00,\n",
       "         7.6145e-01,  1.3519e+00,  4.6614e-01, -4.3710e-01,  1.8321e+00,\n",
       "         4.2737e+00,  2.6064e+00,  1.1261e+00, -5.2822e-01,  1.3723e+00,\n",
       "         1.8639e+00,  1.7397e+00,  2.4431e+00,  9.5692e-01,  3.6457e-01,\n",
       "         1.5476e+00,  2.0092e+00,  1.9175e+00,  3.0049e+00,  1.2039e+00,\n",
       "         1.5750e+00,  1.4783e+00, -2.8429e-01,  1.3193e+00,  2.8215e+00,\n",
       "         3.9818e+00,  2.4481e+00,  2.1238e+00,  2.0031e+00,  1.7739e+00,\n",
       "         4.1456e+00,  1.9561e+00, -7.3679e-01,  5.8964e-01,  1.5699e+00,\n",
       "         1.3692e+00,  1.1410e+00,  8.2037e-01,  1.6963e+00,  2.0439e+00,\n",
       "         2.6174e+00,  5.3471e-01,  1.3227e+00,  2.6537e-01,  3.6169e+00,\n",
       "         2.0835e+00,  3.3525e+00,  1.2594e+00, -5.9824e-01,  2.4229e+00,\n",
       "         1.0757e+00,  8.7420e-01,  1.9898e-01,  2.9266e+00,  4.7172e-01,\n",
       "         1.2634e+00,  2.3828e+00,  1.1833e+00,  2.2257e+00,  4.0382e+00,\n",
       "         1.9378e+00,  3.2285e+00,  1.7228e+00, -5.9223e-02,  2.2015e+00,\n",
       "         3.2444e+00,  2.4665e+00,  1.9718e+00,  2.7750e+00,  2.0344e+00,\n",
       "         3.1158e+00,  1.3561e+00,  1.3464e-01,  2.2513e+00,  9.9139e-01,\n",
       "         1.2819e+00,  5.4603e-01,  2.0059e+00,  4.6016e-01,  1.6168e+00,\n",
       "         2.1326e+00,  8.2513e-01,  1.1606e+00,  3.6254e-01,  2.4949e+00,\n",
       "        -7.3960e-01,  2.3291e+00,  9.3537e-01,  9.9053e-01,  2.2548e+00,\n",
       "        -7.1479e-01,  1.8891e+00,  2.1347e+00,  2.8414e+00, -2.6235e-01,\n",
       "         1.5526e+00,  3.9920e+00,  1.4327e+00,  2.5100e+00,  1.7112e+00,\n",
       "         2.5911e+00,  2.2792e+00,  3.4708e+00, -1.7402e-01,  2.3728e+00,\n",
       "         6.7025e-01,  1.2723e+00,  2.7599e+00,  1.3667e+00,  2.2140e+00,\n",
       "         2.3740e+00,  2.1345e+00,  1.7569e+00, -1.3037e+00,  1.7736e+00,\n",
       "         1.8432e+00,  1.4538e+00,  2.4845e+00,  2.5264e+00,  1.5021e+00,\n",
       "         2.6564e+00,  1.9380e+00,  2.8368e+00,  1.4348e+00,  1.7543e+00,\n",
       "         3.2839e+00,  3.7426e+00,  1.8461e+00,  2.4725e+00,  1.2803e+00,\n",
       "         1.0598e+00,  9.9949e-01,  3.8534e-01,  1.1450e+00,  2.3360e+00])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[0][:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0ab90de4-7b57-4923-8ca9-473dba85e5c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.7386,  1.1559,  4.3890,  1.3708,  3.2283,  1.9757,  1.4864,  0.6046,\n",
       "         2.3884,  1.1410,  3.3498,  1.1096,  3.1578,  4.2387,  0.4782,  2.0617,\n",
       "         1.0176,  2.1076,  2.3509,  2.7729,  1.9457,  2.2885, -0.3038,  1.0378,\n",
       "         0.4936,  2.4642,  3.0313,  2.6664,  1.3960,  0.9151,  2.3526,  1.8357,\n",
       "         2.1642,  2.0526,  3.2073,  1.7686,  1.6532,  3.5243,  3.3730,  1.9575,\n",
       "         0.6292,  2.6592,  2.8368,  2.8270,  2.3554,  3.3087,  2.3161,  1.0955,\n",
       "         4.7291,  2.2746,  0.4864,  1.5052,  1.9754,  3.3743,  1.9575,  3.6084,\n",
       "         2.3448, -0.0455,  2.2315,  3.1405,  0.6969,  3.4210,  2.4723,  1.7862,\n",
       "         1.0641,  0.3242,  3.1868,  1.4764,  2.7546,  2.2155,  1.0719,  3.1571,\n",
       "         2.6292,  1.0948,  2.0707,  1.7225,  2.6315,  2.7319, -1.5639,  0.5955,\n",
       "         3.0438,  1.1719, -0.6786,  3.1406,  0.9944,  0.9930,  1.6354,  1.8626,\n",
       "         2.6885,  2.9180,  2.1727, -0.4518,  2.1431,  2.3909,  3.0199,  2.0408,\n",
       "         1.2558,  3.2367,  1.9096,  1.8839,  1.1976,  3.6060,  0.5917,  1.7025,\n",
       "         1.5310,  1.7752,  1.7177,  1.8113,  2.1705,  3.5489,  2.1238,  2.4157,\n",
       "         1.1926,  2.4400,  0.5055,  2.2869,  1.1239,  1.9040,  1.1524,  0.5724,\n",
       "        -1.9204,  0.2146,  2.0997,  3.8477,  0.6562,  1.0104,  0.9744,  1.2403,\n",
       "         2.5591,  3.5947,  3.3443,  1.0473,  1.1101,  1.4122,  1.2755, -1.0482,\n",
       "        -0.4166,  1.2310,  0.7411,  0.9034,  3.1241,  2.6653,  1.0794,  0.6633,\n",
       "         1.2644,  2.5407,  0.6885,  2.6069, -0.6756,  3.3729,  1.5758,  2.7450,\n",
       "         2.8421,  0.1392,  4.2785,  2.0038,  4.1264,  3.7046,  0.6049,  2.6080,\n",
       "         1.9882,  2.1568,  2.3043,  2.4581,  0.8396,  2.3309,  2.0290, -0.1368,\n",
       "         0.7092,  1.9388,  1.6408,  2.4370,  2.6748,  3.5937,  1.5988,  2.2009,\n",
       "        -0.4829, -1.6592,  1.1713,  1.6655,  2.1375,  2.6772, -0.3212,  1.8958,\n",
       "         1.3748,  2.4938,  3.7724,  3.3954,  2.4534,  2.0558,  2.5334,  2.6174,\n",
       "         3.8729,  3.3157,  1.0706,  2.4170,  1.7548,  1.4982,  3.9479,  2.6057,\n",
       "         2.6235,  1.6353,  1.4619,  1.7212,  2.3933,  1.4523,  4.5443,  0.8825,\n",
       "         1.2830,  1.1572,  2.5438,  2.5727,  1.6025,  3.3770,  2.1385,  2.3935,\n",
       "         2.8448,  2.3417,  1.0228,  1.1778,  2.3756,  2.1683,  0.9741,  3.7548,\n",
       "         2.7932,  1.6918,  2.2969,  1.7179,  2.9506,  2.8978,  2.2992,  3.5294,\n",
       "         1.6540,  2.7998,  1.4050,  1.6139,  2.6354,  1.6560,  2.0126,  3.6929,\n",
       "         1.7397,  2.0875,  0.6530,  0.9668,  2.1345,  1.4125,  0.7164,  1.3979,\n",
       "         0.2872,  1.9982,  1.5888,  1.8220,  1.7649,  2.0969,  2.2515, -0.2838,\n",
       "         2.7632,  1.9191,  1.6868,  2.5100,  1.7648,  2.7155,  1.5075,  3.6247,\n",
       "         2.4493,  1.3467, -0.0937,  1.6956,  3.3268,  1.6934,  2.3562,  1.6065,\n",
       "         0.1460,  1.6442,  3.5198,  1.6457,  3.4332,  3.3385,  1.1761,  2.6130,\n",
       "         2.1276,  1.1438,  0.9007,  3.3940,  3.3272,  2.2185,  2.7501,  1.0731,\n",
       "         2.4032,  1.9204,  0.4047,  2.8501,  1.9046,  1.6166,  1.3980,  3.8485,\n",
       "         2.9539,  1.0873,  1.8179,  2.6885,  3.6220,  3.1535,  2.3883,  1.9595,\n",
       "         1.4760,  3.6616,  0.6555,  1.2924,  0.6540,  1.8112,  2.0062,  0.3231,\n",
       "         2.4568,  0.5809,  1.6861,  2.5886,  2.4502,  1.9084,  0.9420,  2.9381,\n",
       "         0.3122,  0.0229,  2.5323,  0.6417, -1.0522,  0.4134,  2.6766,  2.9974,\n",
       "         1.0365,  2.5928,  2.5763,  2.0133, -1.2156, -0.3654,  1.3482,  2.1815,\n",
       "         0.3922, -0.3926,  1.6691,  2.5849,  1.1986,  1.4396,  0.1758,  0.2772,\n",
       "         2.1997,  2.8228,  2.6775,  1.7217,  0.9423,  2.3369,  2.8458,  2.5435,\n",
       "         1.0035,  2.4579,  3.0049,  2.9997,  1.7713,  0.6755,  1.7628,  1.7021,\n",
       "         0.8994,  1.7514,  0.3465,  1.2460,  1.5638,  0.9183,  3.8610,  1.6142,\n",
       "         1.8161,  3.0854,  2.8805,  1.2429,  2.1996,  3.3574,  4.0273,  1.5534,\n",
       "         1.3959,  1.8903,  2.8313,  0.9439,  2.1816,  2.6041,  1.6223,  2.0496,\n",
       "         2.6707,  2.0449,  2.7488,  1.2666,  3.3344,  2.3335,  2.2479,  2.3339,\n",
       "         2.4968,  4.0442,  3.6284,  1.3746,  1.1808,  2.4042,  2.2700,  2.6969,\n",
       "         3.6322,  2.3672,  1.9215,  1.2457,  3.0066,  1.1816,  1.7853,  1.7728,\n",
       "         3.2474,  1.6908,  1.9142,  1.8743,  2.0854,  2.3673,  1.2909,  2.9202,\n",
       "         1.9559,  2.4651,  3.0991,  2.6570,  1.9585,  3.3728,  3.2510,  0.7072,\n",
       "         0.1412,  4.7577,  1.5358,  2.6657,  2.8659,  2.3826,  0.5087,  0.8616,\n",
       "         3.1133,  2.3101,  2.3910,  3.2360,  2.1460,  2.5727,  2.5347,  0.3210,\n",
       "         1.0956,  2.6603,  2.2570,  2.6719, -0.1525,  3.6980,  2.5565,  3.1143,\n",
       "         2.5926,  2.7059,  1.0014,  2.1475,  1.3579,  0.9228,  2.8466,  3.0059,\n",
       "         4.1465,  2.4722,  1.0670,  2.5172,  0.9289,  4.0280,  1.2127,  3.6958,\n",
       "         3.1776,  3.1136,  2.5909,  2.9745,  2.0784,  3.0954,  2.8100,  2.0380,\n",
       "         1.5028,  2.4747,  3.0543,  2.4272,  2.9095,  0.8400,  1.2913,  4.5028,\n",
       "         1.7627,  1.5098,  3.4118,  1.5776,  3.3470,  1.5941,  0.3512,  2.0232,\n",
       "         0.9684,  2.4905,  2.7989,  1.2382,  2.5035,  3.2870,  3.0751,  1.7514,\n",
       "        -1.3037,  2.2764,  1.5810,  0.1823,  1.9873,  2.4244,  0.7484,  1.0299,\n",
       "         2.5559,  2.3827,  1.8436,  1.8162,  3.6423,  2.5277,  1.9807,  0.7118,\n",
       "         2.6744,  2.1927,  0.8711,  1.9759, -1.1053,  2.3905,  1.7285,  1.5735,\n",
       "         3.4111,  3.9404,  1.0120,  1.3066,  2.9627,  2.0093,  1.8858,  2.1435,\n",
       "         2.1876,  3.2443,  2.5938,  2.6820,  1.8919,  2.1521,  3.4819,  2.9710,\n",
       "         2.5620,  3.0738,  1.7472,  2.9877,  2.1136,  1.0974,  2.2999,  2.5739,\n",
       "         2.6925,  4.3283,  1.5111,  0.6747,  1.7385,  2.4706,  1.4399,  2.8767,\n",
       "         1.6350,  3.7716,  1.3824,  2.8725,  0.0158,  1.8269,  1.8622,  1.7273,\n",
       "         2.8725,  2.6476,  1.8502,  3.4245,  3.5366,  1.1573,  1.4218,  1.6787,\n",
       "         3.8920,  1.2797,  1.6055,  1.8344, -0.1554,  1.3362,  2.0468,  4.0261,\n",
       "         2.0365,  0.6756,  2.8289,  3.7429,  1.0718,  2.6217,  2.3452,  1.6191,\n",
       "         1.0904,  2.0920,  1.8428,  2.6327,  2.0714,  0.5421,  1.7531,  3.3695,\n",
       "         0.3000,  1.3172,  2.4050,  2.9176,  3.7724,  3.0480,  0.7891,  2.2484,\n",
       "         2.1543,  2.1501,  3.2219,  1.4175,  1.0710,  3.1740,  1.8949,  3.2403,\n",
       "         1.1979,  0.8114,  3.6478,  2.1063,  0.6510,  3.3791,  2.5535,  3.9611,\n",
       "         1.8043,  1.6175,  3.0287,  1.1943,  1.5534, -0.8039,  2.4591,  1.9016,\n",
       "        -0.3646,  1.4960,  2.1911,  3.0935,  2.9429,  1.5959,  0.8859,  2.0786,\n",
       "         2.6170, -0.1637,  1.5472,  1.7414,  0.4374,  0.8839,  2.2978,  1.9508,\n",
       "         0.2780,  4.0998,  2.4454,  2.5988,  0.8808,  1.4795,  3.3248,  3.1153,\n",
       "         1.1783,  1.8165,  3.5908,  3.2961,  2.8470,  2.4742,  1.5047,  0.5125,\n",
       "         3.1985,  2.7773,  1.9759,  1.2475,  1.4165,  2.0096,  2.5460,  0.3031,\n",
       "         2.7491,  2.0363,  3.1071,  0.9837, -0.6041,  2.5427,  3.1213,  3.3631,\n",
       "         0.8396,  2.8082,  2.9671,  1.1730,  2.0882,  3.7864,  3.0782,  2.2400,\n",
       "         2.3904,  1.8006,  2.2298,  3.2080,  2.1157,  0.8487,  3.1543,  2.0865,\n",
       "         1.2648,  1.3860,  0.6509,  1.3086,  2.9960, -0.4436,  1.2965,  1.3732,\n",
       "         1.2388,  0.0211,  1.7169,  1.2866,  1.8438,  2.6613,  3.2446,  2.3040,\n",
       "         2.8803,  2.4047,  2.5956,  1.6457,  2.0827,  1.8835,  2.1105,  1.4511,\n",
       "         1.5010,  1.0968,  3.1744,  2.2313,  1.5579, -0.0507,  0.9738, -0.8031,\n",
       "        -0.8551,  1.6425,  2.3354,  1.4552, -0.0336,  3.0225,  3.3437,  2.6075,\n",
       "         3.6564,  2.8311,  1.1258,  2.3944,  2.4689,  1.4930,  3.3034,  1.8631,\n",
       "         2.4231,  4.0852,  2.0578,  2.7965,  3.3021,  3.1916,  1.4965,  1.4305,\n",
       "         3.1922,  0.8734,  1.2506,  0.3387,  2.2650,  1.8597,  1.6927,  2.5185,\n",
       "         4.2690,  1.9701,  0.9321,  2.2557,  2.4426,  1.2986,  2.6481,  3.0201,\n",
       "        -0.3817,  2.4991,  2.9816, -0.2445,  2.7562,  1.7821,  2.2159,  0.8275,\n",
       "         1.3858,  0.6361,  2.0505,  2.6187,  1.9726,  1.3332,  1.7838,  1.9004,\n",
       "         2.1289,  2.8326,  3.2039,  1.2195,  1.8315,  2.2357,  1.5093,  1.8382,\n",
       "         2.0108,  2.7003,  1.8764,  1.4940,  3.6254,  1.1674,  3.1638,  2.8959,\n",
       "         0.7502,  3.0679,  1.9420,  2.0504, -0.9208,  1.7472,  1.8547,  1.1508,\n",
       "         1.9932,  2.3555,  1.8502,  0.8579,  1.7115,  0.4910,  2.6140,  2.6919,\n",
       "         1.7229,  1.6169,  1.5405,  1.1111,  0.4892,  1.0797,  3.6841,  0.4926,\n",
       "         2.4681,  1.0836,  2.4085,  1.9379,  0.8260,  3.9375,  1.5872,  3.1956,\n",
       "         1.2010,  1.7089,  3.1279,  0.7879,  2.0055,  2.5067, -0.7820,  2.4665,\n",
       "         2.3915,  2.7174,  2.4589, -1.2825,  1.2189,  1.4977,  1.3427,  1.6970,\n",
       "         2.2923,  2.2010,  1.1882,  0.3306,  1.6450,  0.6185,  2.6136,  0.4252,\n",
       "         3.4085,  3.2188,  1.4003,  3.3118,  0.9871,  1.2499,  1.2093,  2.0059,\n",
       "         3.2946,  1.4982,  0.5604,  1.2710,  1.4303,  2.4015,  1.5121, -0.8435,\n",
       "         3.3785,  2.0506,  2.8880,  2.0590,  3.6582,  0.9351,  3.8642,  1.3921,\n",
       "         0.4797,  1.4683,  1.9505,  2.1889,  3.2894,  3.5883,  2.5209,  2.7408,\n",
       "         0.5368,  0.7685,  3.2436,  2.4621,  2.7936,  0.2972,  2.6931,  1.0869,\n",
       "         2.7391,  2.5346,  1.8068,  0.3416,  1.2081,  2.3814,  3.5860,  1.6933,\n",
       "         3.5401,  4.1320,  4.1836,  2.5018,  2.1616,  2.7998,  3.6849,  3.1896,\n",
       "         2.9131,  1.8397,  2.2126,  1.4813,  0.5080,  2.7235,  1.0384,  1.7750,\n",
       "         3.1456,  1.9898,  1.6653,  0.2231,  1.3716,  2.3740,  0.7392, -0.6558,\n",
       "         3.2829,  3.6106,  2.0048,  2.6465,  1.5849,  3.2131,  2.5358,  1.4954,\n",
       "         1.0534,  0.7926,  2.8215,  2.3105,  3.3186,  2.3596,  2.1343,  1.7327,\n",
       "         1.9749,  1.9209,  2.3662,  0.9059, -0.3676,  2.7173,  2.0024,  1.0429,\n",
       "         2.2552,  5.5390,  0.9179,  3.4966,  3.1950,  4.0604, -0.1080,  2.6012,\n",
       "         1.5789,  3.2829, -1.0941,  1.6323,  2.2302,  2.3286, -0.5368,  2.3228,\n",
       "         2.9774,  2.1968,  0.9476,  4.1051,  1.5090,  2.8075,  1.5844,  2.2034,\n",
       "         2.1602,  2.1760,  4.2428,  2.7709,  3.9920,  2.7334,  2.1178,  1.6163,\n",
       "         3.3979,  3.5312,  1.0652,  3.0887,  2.5741,  3.2286,  2.0882, -0.3037,\n",
       "        -0.0351,  2.3981,  0.7614,  0.7596,  0.4722,  0.4874,  1.9796,  0.5496,\n",
       "         5.0088,  1.2194,  0.6787,  2.7376,  2.9151,  2.2364,  2.6793,  1.6807])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d3641337-67c1-4bfa-892b-efab3943db18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict from model class\n",
    "# torch.random.manual_seed(8)\n",
    "# loss_dict = model_lightning.predict(\n",
    "#     batch=next(iter(loaders[2])),\n",
    "#     observational=False,\n",
    "#     intervene=False,\n",
    "#     counterfactual=False,\n",
    "#     ate=True,\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e988be17-090d-41b8-a204-2330450410eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0087, -0.0932,  0.0234, -0.0100,  0.7340])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = {}\n",
    "x = batch[0] # is this u or x? I hope x\n",
    "n = x.shape[0]\n",
    "with torch.enable_grad():\n",
    "    output[\"log_prob_true\"] = preparator.log_prob(x)\n",
    "\n",
    "log_prob = model.log_prob(x, scaler=preparator.scaler_transform)\n",
    "output[\"loss\"] = -log_prob\n",
    "output[\"log_prob\"] = log_prob\n",
    "intervention_list = preparator.get_ate_list()\n",
    "\n",
    "int_dict = {'name': '1_0', 'a': 1., 'b': 0., 'index': 4}\n",
    "\n",
    "\n",
    "name = int_dict[\"name\"]\n",
    "a = int_dict[\"a\"]#1.\n",
    "b = int_dict[\"b\"]#0.\n",
    "index = int_dict[\"index\"]\n",
    "\n",
    "ate = model_lightning.model.compute_ate(\n",
    "    index,\n",
    "    a=a,\n",
    "    b=b,\n",
    "    num_samples=10000,\n",
    "    scaler=preparator.scaler_transform,\n",
    ")\n",
    "ate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "872e5308-46b7-41c9-8961-26ae0ab1a733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([-0.02142811,  0.01597714, -0.03495955, -0.0507586 ,  0.7280544 ],\n",
      "      dtype=float32)]\n",
      "[array([-0.02142811,  0.01597714, -0.03495955, -0.0507586 ,  0.7280544 ],\n",
      "      dtype=float32), array([-5.2449703e-03,  1.0221481e-02,  8.2733631e-03,  7.2073936e-04,\n",
      "        7.5292134e-01], dtype=float32)]\n",
      "[array([-0.02142811,  0.01597714, -0.03495955, -0.0507586 ,  0.7280544 ],\n",
      "      dtype=float32), array([-5.2449703e-03,  1.0221481e-02,  8.2733631e-03,  7.2073936e-04,\n",
      "        7.5292134e-01], dtype=float32), array([ 0.02558947, -0.00136757, -0.01175404, -0.00143647,  0.74003637],\n",
      "      dtype=float32)]\n",
      "[array([-0.02142811,  0.01597714, -0.03495955, -0.0507586 ,  0.7280544 ],\n",
      "      dtype=float32), array([-5.2449703e-03,  1.0221481e-02,  8.2733631e-03,  7.2073936e-04,\n",
      "        7.5292134e-01], dtype=float32), array([ 0.02558947, -0.00136757, -0.01175404, -0.00143647,  0.74003637],\n",
      "      dtype=float32), array([0.04610872, 0.06636262, 0.00415158, 0.06623077, 0.7587135 ],\n",
      "      dtype=float32)]\n",
      "[array([-0.02142811,  0.01597714, -0.03495955, -0.0507586 ,  0.7280544 ],\n",
      "      dtype=float32), array([-5.2449703e-03,  1.0221481e-02,  8.2733631e-03,  7.2073936e-04,\n",
      "        7.5292134e-01], dtype=float32), array([ 0.02558947, -0.00136757, -0.01175404, -0.00143647,  0.74003637],\n",
      "      dtype=float32), array([0.04610872, 0.06636262, 0.00415158, 0.06623077, 0.7587135 ],\n",
      "      dtype=float32), array([ 0.00641727, -0.03390503,  0.05474949,  0.00209665,  0.76336896],\n",
      "      dtype=float32)]\n",
      "mean: 0.7486189\n",
      "std: 0.012919243\n"
     ]
    }
   ],
   "source": [
    "# manual implementation of my_predict\n",
    "n_rounds = 5\n",
    "ates = []\n",
    "seeds = np.arange(n_rounds)\n",
    "for i, seed in enumerate(seeds):\n",
    "    output = {}\n",
    "    x = batch[0] # is this u or x? I hope x\n",
    "    n = x.shape[0]\n",
    "    with torch.enable_grad():\n",
    "        output[\"log_prob_true\"] = preparator.log_prob(x)\n",
    "    \n",
    "    log_prob = model.log_prob(x, scaler=preparator.scaler_transform)\n",
    "    output[\"loss\"] = -log_prob\n",
    "    output[\"log_prob\"] = log_prob\n",
    "    # intervention_list = preparator.get_ate_list()\n",
    "    \n",
    "    int_dict = {'name': '1_0', 'a': 1., 'b': 0., 'index': 4}\n",
    "    \n",
    "\n",
    "    name = int_dict[\"name\"]\n",
    "    a = int_dict[\"a\"]#1.\n",
    "    b = int_dict[\"b\"]#0.\n",
    "    index = int_dict[\"index\"]\n",
    "\n",
    "    torch.random.manual_seed(seed) \n",
    "    ate = model_lightning.model.compute_ate(\n",
    "        index,\n",
    "        a=a,\n",
    "        b=b,\n",
    "        num_samples=10000,\n",
    "        scaler=preparator.scaler_transform,\n",
    "    )\n",
    "    \n",
    "    ates.append(ate.detach().numpy())\n",
    "    print(ates)\n",
    "\n",
    "ates = np.array(ates)\n",
    "ates_y = ates[:,-1]\n",
    "mean = ates_y.mean()\n",
    "std =  ates_y.std()\n",
    "print('mean:', mean)\n",
    "print('std:', std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e569b90b-b571-4b0f-b4de-c947162cf395",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_predict(\n",
    "        self,\n",
    "        batch,\n",
    "        observational=False,\n",
    "        intervene=False,\n",
    "        counterfactual=False,\n",
    "        ate=False,\n",
    "        intervention_list=None,\n",
    "    ):\n",
    "        output = {}\n",
    "        x = batch[0].to(self.device)\n",
    "        n = x.shape[0]\n",
    "        with torch.enable_grad():\n",
    "            output[\"log_prob_true\"] = self.preparator.log_prob(x)\n",
    "\n",
    "        tic = time.time()\n",
    "        log_prob = self.model.log_prob(x, scaler=self.preparator.scaler_transform)\n",
    "        output[\"time_log_prob\"] = self.compute_time(tic, n)\n",
    "        output[\"loss\"] = -log_prob\n",
    "        output[\"log_prob\"] = log_prob\n",
    "\n",
    "        if ate:\n",
    "            if intervention_list is None:\n",
    "                intervention_list = self.preparator.get_ate_list()\n",
    "            delta_times = []\n",
    "            for int_dict in intervention_list:\n",
    "                name = int_dict[\"name\"]\n",
    "                a = int_dict[\"a\"]\n",
    "                b = int_dict[\"b\"]\n",
    "                index = int_dict[\"index\"]\n",
    "                tic = time.time()\n",
    "                ate = self.model.compute_ate(\n",
    "                    index,\n",
    "                    a=a,\n",
    "                    b=b,\n",
    "                    num_samples=10000,\n",
    "                    scaler=self.preparator.scaler_transform,\n",
    "                )\n",
    "                delta_times.append(self.compute_time(tic, 10000))\n",
    "\n",
    "                ate_true = self.preparator.compute_ate(\n",
    "                    index, a=a, b=b, num_samples=10000\n",
    "                )\n",
    "\n",
    "                diff_ate = ate_true - ate\n",
    "\n",
    "                rmse = torch.sqrt((diff_ate**2).sum())\n",
    "                output[f\"rmse_ate_x{index + 1}={name}\"] = rmse\n",
    "\n",
    "            delta_time = torch.stack(delta_times).mean()\n",
    "            output[\"time_ate\"] = delta_time\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6644606e-ecea-4ff4-9ae1-df35deb1e043",
   "metadata": {},
   "outputs": [],
   "source": [
    "intervention_list = [{'name': '1_0', 'a': 1., 'b': 0., 'index': 0},]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2a6d5998-d638-4e59-a81d-c37babf7478d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'log_prob_true': tensor([-2.8672, -2.4891, -6.4312,  ..., -4.2171, -2.1910, -1.8383]),\n",
       " 'time_log_prob': tensor(0.9432),\n",
       " 'loss': tensor([2.5407, 2.3248, 7.3806,  ..., 4.2023, 2.1255, 2.3977],\n",
       "        grad_fn=<NegBackward0>),\n",
       " 'log_prob': tensor([-2.5407, -2.3248, -7.3806,  ..., -4.2023, -2.1255, -2.3977],\n",
       "        grad_fn=<AddBackward0>),\n",
       " 'rmse_ate_x1=1_0': tensor(0.0135),\n",
       " 'time_ate': tensor(4.4213)}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bri = my_predict(\n",
    "        self=model_lightning,\n",
    "        batch=batch,\n",
    "        observational=False,\n",
    "        intervene=False,\n",
    "        counterfactual=False,\n",
    "        ate=True,\n",
    "        intervention_list=intervention_list,\n",
    "    )\n",
    "bri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f492db-bf45-4618-b3bb-e51094068498",
   "metadata": {},
   "outputs": [],
   "source": [
    "python main.py --config_file causal_nf/configs/FF_configs.yaml --wandb_mode disabled --project FF_data\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
